import streamlit as st
import os
from langchain_ollama import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from retrieval.retriever import retrieve_docs, build_context
from generation.generator import generate_answer
from generation.prompt import PROMPT_TEMPLATE
import dotenv
from langchain_openai import ChatOpenAI
import os
from evaluation.rag_evaluator import evaluate_rag_system


# --- 1. é¡µé¢é…ç½®ä¸è‡ªå®šä¹‰æ ·å¼ ---
st.set_page_config(page_title="åŒ—äº¤å¤§å­¦ç”Ÿæ‰‹å†ŒåŠ©æ‰‹", page_icon="ğŸ«", layout="centered")

# æ³¨å…¥ CSS ä¿®æ”¹ä¾§è¾¹æ é¢œè‰²ä¸ºæ·±è“è‰² (#003366 æ˜¯æ·±è“ç³»é¢œè‰²)
# --- 1. é¡µé¢é…ç½®ä¸è‡ªå®šä¹‰æ ·å¼ ---
st.set_page_config(page_title="åŒ—äº¤å¤§å­¦ç”Ÿæ‰‹å†ŒåŠ©æ‰‹", page_icon="ğŸ«", layout="centered")

# æ³¨å…¥ CSSï¼šä¿®æ”¹ä¾§è¾¹æ èƒŒæ™¯ä¸ºæ·±è“ï¼Œå¹¶ç¡®ä¿æŒ‰é’®æ–‡å­—ä¸ºé»‘è‰²
st.markdown("""
    <style>
        /* ä¿®æ”¹ä¾§è¾¹æ èƒŒæ™¯é¢œè‰² */
        [data-testid="stSidebar"] {
            background-color: #003366;
        }

        /* ä¿®æ”¹ä¾§è¾¹æ å†…çš„æ ‡é¢˜å’Œæ™®é€šæ–‡å­—ä¸ºç™½è‰² */
        [data-testid="stSidebar"] .stText, 
        [data-testid="stSidebar"] p, 
        [data-testid="stSidebar"] h1, 
        [data-testid="stSidebar"] h2, 
        [data-testid="stSidebar"] h3,
        [data-testid="stSidebar"] span {
            color: white !important;
        }

        /* æ ¸å¿ƒä¿®æ”¹ï¼šå¼ºåˆ¶ä¾§è¾¹æ æŒ‰é’®èƒŒæ™¯ä¸ºçº¯ç™½ï¼Œå­—ä½“é¢œè‰²ä¸ºé»‘è‰² */
        [data-testid="stSidebar"] .stButton button {
            background-color: #ffffff !important;
            color: #000000 !important;
            border: none;
            font-weight: bold;
        }

        /* é¼ æ ‡æ‚¬åœåœ¨æŒ‰é’®ä¸Šæ—¶çš„æ•ˆæœï¼ˆå¯é€‰ï¼Œå¢åŠ äº¤äº’æ„Ÿï¼‰ */
        [data-testid="stSidebar"] .stButton button:hover {
            background-color: #eeeeee !important;
            color: #000000 !important;
        }
    </style>
    """, unsafe_allow_html=True)

# --- 2. åŠ è½½åç«¯æ¨¡å‹ (ä¿æŒä¸å˜) ---
@st.cache_resource
def init_models():
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-small-zh-v1.5",
        model_kwargs={'device': 'cpu'}
    )
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    db_path = os.path.join(BASE_DIR, "chroma_db")
    if not os.path.exists(db_path):
        return None, None
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)

    dotenv.load_dotenv()  #åŠ è½½å½“å‰ç›®å½•ä¸‹çš„ .env æ–‡ä»¶
    os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY")
    os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")
    # åˆ›å»ºå¤§æ¨¡å‹å®ä¾‹
    llm = ChatOpenAI(model="gpt-4o-mini")  # é»˜è®¤ä½¿ç”¨
    # llm = OllamaLLM(model="deepseek-r1:8b", temperature=0.1)
    return vector_db, llm


vector_db, llm = init_models()

# --- 3. é¡µé¢æ ‡é¢˜ ---
st.title("ğŸ« åŒ—äº¬äº¤é€šå¤§å­¦")
st.subheader("å­¦ç”Ÿæ‰‹å†Œæ™ºèƒ½å’¨è¯¢åŠ©æ‰‹")
st.markdown("---")

# --- 4. ä¾§è¾¹æ å†…å®¹ (æ·±è“èƒŒæ™¯) ---
with st.sidebar:
    # å»ºè®®ï¼šå¦‚æœä½ æœ¬åœ°æœ‰æ ¡å¾½å›¾ç‰‡ï¼Œå¯ä»¥ç”¨ st.image("logo.png")
    # è¿™é‡Œå…ˆç”¨åŒ—äº¤å¤§å®˜ç½‘çš„é€æ˜åº•æ ¡å¾½é“¾æ¥ï¼ˆå¦‚æœé“¾æ¥å¤±æ•ˆè¯·æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„ï¼‰
    st.image("https://www.bjtu.edu.cn/images/logo.png", use_container_width=True)

    st.markdown("### ç³»ç»Ÿæ¦‚å†µ")
    st.write("æ¬¢è¿ä½¿ç”¨ï¼æœ¬åŠ©æ‰‹åŸºäº DeepSeek-R1 æ¨ç†æ¨¡å‹ï¼Œä¸“é—¨ä¸ºæ‚¨è§£ç­”ã€Šå­¦ç”Ÿæ‰‹å†Œã€‹ç›¸å…³é—®é¢˜ã€‚")

    st.markdown("---")
    st.info("ğŸ’¡ **æç¤º**ï¼šå…³äºç»©ç‚¹ã€å¥–å­¦é‡‘ã€å¤„åˆ†çš„è§„å®šï¼Œç³»ç»Ÿå·²æ·±åº¦å­¦ä¹ ã€‚")

    if st.button("æ¸…ç©ºå¯¹è¯è®°å½•"):
        st.session_state.messages = []
        st.rerun()

    st.markdown("---")
    st.markdown("### ğŸ“Š ç³»ç»Ÿè¯„ä¼°")

    if st.button("è¿è¡Œ RAG è¯„ä¼°"):
        if len(st.session_state.eval_buffer) == 0:
            st.warning("âš ï¸ å½“å‰è¿˜æ²¡æœ‰å¯è¯„ä¼°çš„é—®ç­”è®°å½•")
        else:
            with st.spinner("æ­£åœ¨è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ï¼Œè¯·ç¨å€™..."):
                try:
                    eval_result = evaluate_rag_system(
                        eval_records=st.session_state.eval_buffer,
                        llm=llm,
                        embeddings=vector_db._embedding_function,
                    )
                    st.success("è¯„ä¼°å®Œæˆï¼")
                    df = eval_result.to_pandas()
                    st.dataframe(
                        df[
                            [
                                "nv_context_relevance",
                                "answer_relevancy",
                                "faithfulness",
                                "nv_response_groundedness",
                            ]
                        ],
                        use_container_width=True,
                    )

                except Exception as e:
                    st.error(f"è¯„ä¼°å¤±è´¥ï¼š{e}")

# --- 5. å¯¹è¯é€»è¾‘ (ä¿æŒä¸å˜) ---
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "åŒå­¦ä½ å¥½ï¼æˆ‘æ˜¯åŒ—äº¬äº¤é€šå¤§å­¦å­¦ç”Ÿæ‰‹å†Œå°åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"}
    ]

if "eval_buffer" not in st.session_state:
    st.session_state.eval_buffer = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if user_query := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": user_query})
    with st.chat_message("user"):
        st.markdown(user_query)

    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨æ£€ç´¢æ ¡è§„å¹¶æ€è€ƒ..."):
            try:
                # â‘  æ£€ç´¢
                docs = retrieve_docs(vector_db, user_query)
                # â‘¡ æ„å»ºä¸Šä¸‹æ–‡
                context = build_context(docs)
                # â‘¢ ç”Ÿæˆå›ç­”
                answer = generate_answer(
                    llm,
                    PROMPT_TEMPLATE,
                    user_query,
                    context
                )
                st.markdown(answer)
                st.session_state.messages.append(
                    {"role": "assistant", "content": answer}
                )
                # â‘£ è¯„ä¼°æŒ‡æ ‡
                st.session_state.eval_buffer.append({
                    "query": user_query,
                    "answer": answer,
                    "contexts": [doc.page_content for doc in docs],
                })
            except Exception as e:
                st.error(f"å‡ºé”™å•¦: {e}")